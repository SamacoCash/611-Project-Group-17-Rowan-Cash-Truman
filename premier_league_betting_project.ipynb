{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db43153",
   "metadata": {},
   "source": [
    "# Premier League Betting App – Match Outcome & Score Prediction\n",
    "\n",
    "This notebook implements the pipeline described in the project proposal:\n",
    "\n",
    "- **Client:** Premier League Betting App  \n",
    "- **Goal:**  \n",
    "  - Classification: Predict the **match outcome**  \n",
    "    - `0` = Home Win, `1` = Away Win, `2` = Draw  \n",
    "  - Regression: Predict the **final score** (home & away goals).  \n",
    "- **Data:** Premier League matchup stats (e.g., Kaggle datasets for seasons 2019/2020, 2020/2021, 2021/2022) concatenated into a single dataset.  \n",
    "- **Models:**  \n",
    "  - **Classification:** SVM, Random Forest, Logistic Regression  \n",
    "  - **Regression:** Linear Regression (with regularization), Random Forest Regressor, Gradient Boosting Regressor  \n",
    "- **Framework:** Preprocessing → Data splitting → Hyperparameter tuning → Model training → Validation → Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd9771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing & model selection\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Classification models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Regression models\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    mean_squared_error, r2_score\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca349100",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "Update the file paths below to point to your downloaded Kaggle CSV files for the **Premier League** seasons\n",
    "2019/2020, 2020/2021, and 2021/2022.\n",
    "\n",
    "You may have columns like:\n",
    "\n",
    "- `HomeTeam`, `AwayTeam`\n",
    "- `FTHG` (Full-Time Home Goals), `FTAG` (Full-Time Away Goals)\n",
    "- `FTR` (Full-Time Result) as `'H'`, `'A'`, `'D'`\n",
    "\n",
    "You can adjust the column names in the preprocessing steps later if your dataset uses different ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373f28a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update these paths to match your local dataset files\n",
    "path_2019_2020 = \"data/epl_2019_2020.csv\"\n",
    "path_2020_2021 = \"data/epl_2020_2021.csv\"\n",
    "path_2021_2022 = \"data/epl_2021_2022.csv\"\n",
    "\n",
    "# Load datasets\n",
    "df_19_20 = pd.read_csv(path_2019_2020)\n",
    "df_20_21 = pd.read_csv(path_2020_2021)\n",
    "df_21_22 = pd.read_csv(path_2021_2022)\n",
    "\n",
    "# Concatenate datasets\n",
    "df = pd.concat([df_19_20, df_20_21, df_21_22], ignore_index=True)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78a29b8",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Quick sanity checks: data types, missing values, and simple distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6339d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of the dataset\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47d8416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check basic statistics for numerical columns\n",
    "df.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0028f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "df.isna().mean().sort_values(ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b553a2e6",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering & Target Definition\n",
    "\n",
    "We define:\n",
    "\n",
    "- **Classification target `y_cls`**: Match outcome encoded as  \n",
    "  - `0` = Home Win  \n",
    "  - `1` = Away Win  \n",
    "  - `2` = Draw  \n",
    "\n",
    "- **Regression targets `y_reg_home` and `y_reg_away`**: Final scores (home & away goals).\n",
    "\n",
    "Adjust the column names in this section if your dataset uses different labels for goals and result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a28cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Adjust these column names to match your dataset ----\n",
    "home_team_col = \"HomeTeam\"\n",
    "away_team_col = \"AwayTeam\"\n",
    "home_goals_col = \"FTHG\"  # Full Time Home Goals\n",
    "away_goals_col = \"FTAG\"  # Full Time Away Goals\n",
    "result_col = \"FTR\"       # Full Time Result: 'H', 'A', 'D'\n",
    "\n",
    "# Map textual result to numeric class: 0=Home Win, 1=Away Win, 2=Draw\n",
    "result_mapping = {\"H\": 0, \"A\": 1, \"D\": 2}\n",
    "df = df.dropna(subset=[home_goals_col, away_goals_col, result_col])\n",
    "df[\"match_outcome\"] = df[result_col].map(result_mapping)\n",
    "\n",
    "# Regression targets\n",
    "df[\"home_score\"] = df[home_goals_col]\n",
    "df[\"away_score\"] = df[away_goals_col]\n",
    "\n",
    "# Example feature set: you can expand this with more stats from your dataset\n",
    "# For now, let's include team names + any other numeric stats that might exist.\n",
    "feature_cols_categorical = [home_team_col, away_team_col]\n",
    "feature_cols_numeric = [\n",
    "    col for col in df.columns\n",
    "    if col not in feature_cols_categorical\n",
    "    and col not in [home_goals_col, away_goals_col, result_col, \"match_outcome\", \"home_score\", \"away_score\"]\n",
    "    and pd.api.types.is_numeric_dtype(df[col])\n",
    "]\n",
    "\n",
    "print(\"Categorical features:\", feature_cols_categorical)\n",
    "print(\"Numeric features:\", feature_cols_numeric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abda781c",
   "metadata": {},
   "source": [
    "## 4. Train–Test Split\n",
    "\n",
    "We split the data into:\n",
    "\n",
    "- **Training set:** 80%  \n",
    "- **Test set:** 20%  \n",
    "\n",
    "We will perform **5-fold cross-validation** on the training set during hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4f55bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Shuffle to reduce temporal bias if data is ordered by date\n",
    "df = shuffle(df, random_state=42).reset_index(drop=True)\n",
    "\n",
    "X = df[feature_cols_categorical + feature_cols_numeric]\n",
    "\n",
    "# Classification target\n",
    "y_cls = df[\"match_outcome\"]\n",
    "\n",
    "# Regression targets\n",
    "y_reg_home = df[\"home_score\"]\n",
    "y_reg_away = df[\"away_score\"]\n",
    "\n",
    "X_train, X_test, y_cls_train, y_cls_test, y_reg_home_train, y_reg_home_test, y_reg_away_train, y_reg_away_test = train_test_split(\n",
    "    X, y_cls, y_reg_home, y_reg_away, test_size=0.2, random_state=42, stratify=y_cls\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba63fc9d",
   "metadata": {},
   "source": [
    "## 5. Preprocessing Pipelines\n",
    "\n",
    "We apply the following preprocessing steps:\n",
    "\n",
    "- **Missing values:** `SimpleImputer` with mean (numeric) or most frequent (categorical)  \n",
    "- **Scaling:** `StandardScaler` for numeric features  \n",
    "- **Encoding:** `OneHotEncoder` for categorical features  \n",
    "\n",
    "We build a `ColumnTransformer` to apply the correct preprocessing to each subset of columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3684ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric preprocessing: impute missing values with mean, then scale\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Categorical preprocessing: impute most frequent, then one-hot encode\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combined preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, feature_cols_numeric),\n",
    "        (\"cat\", categorical_transformer, feature_cols_categorical),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f260f7c0",
   "metadata": {},
   "source": [
    "## 6. Classification – Match Outcome\n",
    "\n",
    "We train and tune:\n",
    "\n",
    "- **SVM (SVC)** – tuning `kernel`, `C`, `gamma`  \n",
    "- **RandomForestClassifier** – tuning `n_estimators`, `max_depth`, `min_samples_split`  \n",
    "- **LogisticRegression** – tuning `penalty` (L1/L2) and `C`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3822ba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_results = {}\n",
    "\n",
    "# Helper function to run GridSearchCV and store results\n",
    "def run_classification_grid_search(model, param_grid, model_name):\n",
    "    pipe = Pipeline(steps=[(\"preprocess\", preprocessor), (\"model\", model)])\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        pipe,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring=\"accuracy\",\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "    )\n",
    "    grid.fit(X_train, y_cls_train)\n",
    "    \n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_cls_test, y_pred)\n",
    "    prec = precision_score(y_cls_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "    rec = recall_score(y_cls_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(y_cls_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "    \n",
    "    classification_results[model_name] = {\n",
    "        \"best_params\": grid.best_params_,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"best_model\": best_model,\n",
    "        \"y_pred\": y_pred,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(\"Best params:\", grid.best_params_)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision (weighted): {prec:.4f}\")\n",
    "    print(f\"Recall (weighted): {rec:.4f}\")\n",
    "    print(f\"F1-score (weighted): {f1:.4f}\")\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_cls_test, y_pred, zero_division=0))\n",
    "    \n",
    "    return best_model, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e34a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 SVM (SVC)\n",
    "svm_param_grid = {\n",
    "    \"model__kernel\": [\"rbf\", \"linear\"],\n",
    "    \"model__C\": [0.1, 1, 10],\n",
    "    \"model__gamma\": [\"scale\", \"auto\"],\n",
    "}\n",
    "\n",
    "svm_model, svm_y_pred = run_classification_grid_search(\n",
    "    SVC(probability=True),\n",
    "    svm_param_grid,\n",
    "    model_name=\"SVM\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b29880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Random Forest Classifier\n",
    "rf_cls_param_grid = {\n",
    "    \"model__n_estimators\": [100, 200],\n",
    "    \"model__max_depth\": [None, 10, 20],\n",
    "    \"model__min_samples_split\": [2, 5],\n",
    "}\n",
    "\n",
    "rf_cls_model, rf_cls_y_pred = run_classification_grid_search(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    rf_cls_param_grid,\n",
    "    model_name=\"RandomForestClassifier\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181dca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Logistic Regression\n",
    "log_reg_param_grid = {\n",
    "    \"model__penalty\": [\"l1\", \"l2\"],\n",
    "    \"model__C\": [0.1, 1, 10],\n",
    "    \"model__solver\": [\"liblinear\"],  # supports L1 and L2\n",
    "}\n",
    "\n",
    "log_reg_model, log_reg_y_pred = run_classification_grid_search(\n",
    "    LogisticRegression(max_iter=1000, multi_class=\"auto\"),\n",
    "    log_reg_param_grid,\n",
    "    model_name=\"LogisticRegression\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39265efe",
   "metadata": {},
   "source": [
    "### 6.4 Confusion Matrices & Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2696319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_for_model(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    im = ax.imshow(cm, interpolation=\"nearest\")\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(\n",
    "        xticks=range(3),\n",
    "        yticks=range(3),\n",
    "        xticklabels=[\"Home Win (0)\", \"Away Win (1)\", \"Draw (2)\"],\n",
    "        yticklabels=[\"Home Win (0)\", \"Away Win (1)\", \"Draw (2)\"],\n",
    "        ylabel=\"True label\",\n",
    "        xlabel=\"Predicted label\",\n",
    "        title=title,\n",
    "    )\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Annotate\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(\n",
    "                j, i, format(cm[i, j], \"d\"),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "            )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for name, res in classification_results.items():\n",
    "    plot_confusion_matrix_for_model(y_cls_test, res[\"y_pred\"], f\"Confusion Matrix – {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197b6475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare classification models by metrics\n",
    "cls_summary = pd.DataFrame(\n",
    "    {\n",
    "        name: {\n",
    "            \"accuracy\": res[\"accuracy\"],\n",
    "            \"precision_weighted\": res[\"precision\"],\n",
    "            \"recall_weighted\": res[\"recall\"],\n",
    "            \"f1_weighted\": res[\"f1\"],\n",
    "        }\n",
    "        for name, res in classification_results.items()\n",
    "    }\n",
    ").T\n",
    "\n",
    "cls_summary.sort_values(\"accuracy\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07efc838",
   "metadata": {},
   "source": [
    "## 7. Regression – Final Scores\n",
    "\n",
    "We train and tune:\n",
    "\n",
    "- **Linear Regression with regularization**: Ridge (L2) and Lasso (L1)  \n",
    "- **RandomForestRegressor** – tuning `n_estimators`, `max_depth`  \n",
    "- **GradientBoostingRegressor** – tuning `n_estimators`, `max_depth`, `learning_rate`  \n",
    "\n",
    "We build **separate models** for home and away scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d0c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_results_home = {}\n",
    "regression_results_away = {}\n",
    "\n",
    "def run_regression_grid_search(base_model, param_grid, model_name, y_train, y_test, target_label, results_dict):\n",
    "    pipe = Pipeline(steps=[(\"preprocess\", preprocessor), (\"model\", base_model)])\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        pipe,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "    )\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results_dict[model_name] = {\n",
    "        \"best_params\": grid.best_params_,\n",
    "        \"rmse\": rmse,\n",
    "        \"r2\": r2,\n",
    "        \"best_model\": best_model,\n",
    "        \"y_pred\": y_pred,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n[{target_label}] Model: {model_name}\")\n",
    "    print(\"Best params:\", grid.best_params_)\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R^2: {r2:.4f}\")\n",
    "    \n",
    "    return best_model, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e8ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Ridge Regression – Home score\n",
    "ridge_param_grid = {\n",
    "    \"model__alpha\": [0.1, 1.0, 10.0],\n",
    "}\n",
    "\n",
    "ridge_home_model, ridge_home_pred = run_regression_grid_search(\n",
    "    Ridge(),\n",
    "    ridge_param_grid,\n",
    "    model_name=\"Ridge\",\n",
    "    y_train=y_reg_home_train,\n",
    "    y_test=y_reg_home_test,\n",
    "    target_label=\"Home Score\",\n",
    "    results_dict=regression_results_home,\n",
    ")\n",
    "\n",
    "# 7.2 Lasso Regression – Home score\n",
    "lasso_param_grid = {\n",
    "    \"model__alpha\": [0.001, 0.01, 0.1, 1.0],\n",
    "}\n",
    "\n",
    "lasso_home_model, lasso_home_pred = run_regression_grid_search(\n",
    "    Lasso(max_iter=10000),\n",
    "    lasso_param_grid,\n",
    "    model_name=\"Lasso\",\n",
    "    y_train=y_reg_home_train,\n",
    "    y_test=y_reg_home_test,\n",
    "    target_label=\"Home Score\",\n",
    "    results_dict=regression_results_home,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ed8a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 RandomForestRegressor – Home score\n",
    "rf_reg_param_grid = {\n",
    "    \"model__n_estimators\": [100, 200],\n",
    "    \"model__max_depth\": [None, 10, 20],\n",
    "}\n",
    "\n",
    "rf_home_model, rf_home_pred = run_regression_grid_search(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    rf_reg_param_grid,\n",
    "    model_name=\"RandomForestRegressor\",\n",
    "    y_train=y_reg_home_train,\n",
    "    y_test=y_reg_home_test,\n",
    "    target_label=\"Home Score\",\n",
    "    results_dict=regression_results_home,\n",
    ")\n",
    "\n",
    "# 7.4 GradientBoostingRegressor – Home score\n",
    "gb_reg_param_grid = {\n",
    "    \"model__n_estimators\": [100, 200],\n",
    "    \"model__learning_rate\": [0.05, 0.1],\n",
    "    \"model__max_depth\": [2, 3],\n",
    "}\n",
    "\n",
    "gb_home_model, gb_home_pred = run_regression_grid_search(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    gb_reg_param_grid,\n",
    "    model_name=\"GradientBoostingRegressor\",\n",
    "    y_train=y_reg_home_train,\n",
    "    y_test=y_reg_home_test,\n",
    "    target_label=\"Home Score\",\n",
    "    results_dict=regression_results_home,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e0b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for AWAY score\n",
    "\n",
    "# Ridge – Away score\n",
    "ridge_away_model, ridge_away_pred = run_regression_grid_search(\n",
    "    Ridge(),\n",
    "    ridge_param_grid,\n",
    "    model_name=\"Ridge\",\n",
    "    y_train=y_reg_away_train,\n",
    "    y_test=y_reg_away_test,\n",
    "    target_label=\"Away Score\",\n",
    "    results_dict=regression_results_away,\n",
    ")\n",
    "\n",
    "# Lasso – Away score\n",
    "lasso_away_model, lasso_away_pred = run_regression_grid_search(\n",
    "    Lasso(max_iter=10000),\n",
    "    lasso_param_grid,\n",
    "    model_name=\"Lasso\",\n",
    "    y_train=y_reg_away_train,\n",
    "    y_test=y_reg_away_test,\n",
    "    target_label=\"Away Score\",\n",
    "    results_dict=regression_results_away,\n",
    ")\n",
    "\n",
    "# RandomForestRegressor – Away score\n",
    "rf_away_model, rf_away_pred = run_regression_grid_search(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    rf_reg_param_grid,\n",
    "    model_name=\"RandomForestRegressor\",\n",
    "    y_train=y_reg_away_train,\n",
    "    y_test=y_reg_away_test,\n",
    "    target_label=\"Away Score\",\n",
    "    results_dict=regression_results_away,\n",
    ")\n",
    "\n",
    "# GradientBoostingRegressor – Away score\n",
    "gb_away_model, gb_away_pred = run_regression_grid_search(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    gb_reg_param_grid,\n",
    "    model_name=\"GradientBoostingRegressor\",\n",
    "    y_train=y_reg_away_train,\n",
    "    y_test=y_reg_away_test,\n",
    "    target_label=\"Away Score\",\n",
    "    results_dict=regression_results_away,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c9f2e",
   "metadata": {},
   "source": [
    "### 7.5 Regression Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05382342",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_reg_summary = pd.DataFrame(\n",
    "    {\n",
    "        name: {\"rmse\": res[\"rmse\"], \"r2\": res[\"r2\"]}\n",
    "        for name, res in regression_results_home.items()\n",
    "    }\n",
    ").T\n",
    "\n",
    "away_reg_summary = pd.DataFrame(\n",
    "    {\n",
    "        name: {\"rmse\": res[\"rmse\"], \"r2\": res[\"r2\"]}\n",
    "        for name, res in regression_results_away.items()\n",
    "    }\n",
    ").T\n",
    "\n",
    "print(\"Home score regression summary:\")\n",
    "display(home_reg_summary.sort_values(\"rmse\"))\n",
    "\n",
    "print(\"\\nAway score regression summary:\")\n",
    "display(away_reg_summary.sort_values(\"rmse\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f994ab",
   "metadata": {},
   "source": [
    "## 8. Visualization\n",
    "\n",
    "### 8.1 Predicted vs Actual Score Scatter Plots\n",
    "\n",
    "We use the **best-performing regression models** for home and away scores and visualize predicted vs actual values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edac0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_vs_actual(y_true, y_pred, title):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.scatter(y_true, y_pred, alpha=0.6)\n",
    "    min_val = min(y_true.min(), y_pred.min())\n",
    "    max_val = max(y_true.max(), y_pred.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val])\n",
    "    ax.set_xlabel(\"Actual\")\n",
    "    ax.set_ylabel(\"Predicted\")\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Choose the model with lowest RMSE for home and away\n",
    "best_home_model_name = min(regression_results_home, key=lambda k: regression_results_home[k][\"rmse\"])\n",
    "best_away_model_name = min(regression_results_away, key=lambda k: regression_results_away[k][\"rmse\"])\n",
    "\n",
    "best_home_pred = regression_results_home[best_home_model_name][\"y_pred\"]\n",
    "best_away_pred = regression_results_away[best_away_model_name][\"y_pred\"]\n",
    "\n",
    "print(\"Best home score model:\", best_home_model_name)\n",
    "print(\"Best away score model:\", best_away_model_name)\n",
    "\n",
    "plot_predicted_vs_actual(y_reg_home_test, best_home_pred, f\"Home Score – Predicted vs Actual ({best_home_model_name})\")\n",
    "plot_predicted_vs_actual(y_reg_away_test, best_away_pred, f\"Away Score – Predicted vs Actual ({best_away_model_name})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e06c40",
   "metadata": {},
   "source": [
    "### 8.2 Outcome Prediction Accuracy by Score Margin\n",
    "\n",
    "We can also examine how often the **predicted outcome** (from the best classification model) is correct\n",
    "as a function of the **true goal difference** (score margin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8ad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick best classification model by accuracy\n",
    "best_cls_model_name = max(classification_results, key=lambda k: classification_results[k][\"accuracy\"])\n",
    "best_cls_pred = classification_results[best_cls_model_name][\"y_pred\"]\n",
    "\n",
    "print(\"Best classification model:\", best_cls_model_name)\n",
    "\n",
    "# Compute true score margin (home - away) and absolute margin\n",
    "true_home = y_reg_home_test.reset_index(drop=True)\n",
    "true_away = y_reg_away_test.reset_index(drop=True)\n",
    "true_margin = true_home - true_away\n",
    "abs_margin = true_margin.abs()\n",
    "\n",
    "# Accuracy per margin bin\n",
    "bins = [0, 1, 2, 3, 5, np.inf]\n",
    "labels = [\"0\", \"1\", \"2\", \"3-4\", \"5+\"]\n",
    "margin_bins = pd.cut(abs_margin, bins=bins, labels=labels, right=False)\n",
    "\n",
    "correct = (best_cls_pred == y_cls_test.reset_index(drop=True)).astype(int)\n",
    "accuracy_by_margin = correct.groupby(margin_bins).mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.bar(accuracy_by_margin.index.astype(str), accuracy_by_margin.values)\n",
    "ax.set_xlabel(\"Absolute Goal Difference (True)\")\n",
    "ax.set_ylabel(\"Outcome Prediction Accuracy\")\n",
    "ax.set_title(\"Outcome Prediction Accuracy by Score Margin\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "accuracy_by_margin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4cc7b",
   "metadata": {},
   "source": [
    "## 9. Conclusion & Next Steps\n",
    "\n",
    "In this notebook we:\n",
    "\n",
    "1. Loaded and concatenated multiple Premier League seasons (2019/2020–2021/2022).  \n",
    "2. Preprocessed the data with imputation, scaling, and one-hot encoding.  \n",
    "3. Trained and tuned **classification models** (SVM, Random Forest, Logistic Regression) to predict match outcomes.  \n",
    "4. Trained and tuned **regression models** (Ridge, Lasso, RandomForestRegressor, GradientBoostingRegressor) to predict final scores.  \n",
    "5. Evaluated models using appropriate metrics:  \n",
    "   - **Classification:** Accuracy, Precision, Recall, F1-score  \n",
    "   - **Regression:** RMSE, R²  \n",
    "6. Visualized confusion matrices, predicted vs actual scores, and outcome accuracy by score margin.\n",
    "\n",
    "**Possible extensions:**\n",
    "\n",
    "- Incorporate more advanced features (form, xG, home/away streaks, betting odds).  \n",
    "- Use time-aware validation (e.g., train on earlier seasons, test on later).  \n",
    "- Try more advanced models (XGBoost, LightGBM, neural networks).  \n",
    "- Calibrate predicted probabilities for betting strategies.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
